{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoEncoderTest.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gilgarad/deep_learning/blob/master/code_tests/AutoEncoderTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jsD6YZ-U0m17",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 참조: https://keraskorea.github.io/posts/2018-10-23-keras_autoencoder/\n",
        "# This is mostly copied from someone else, the purpose is to understand and test (could be changed a bit)"
      ]
    },
    {
      "metadata": {
        "id": "GQf_nsh3u6zt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "from keras import regularizers\n",
        "\n",
        "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
        "import keras.backend.tensorflow_backend as K\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Matplotlib 사용\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nR0gbCNLvLQy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "      \n",
        "# Make all data to be between 0 to 1 \n",
        "x_train = x_train / 255\n",
        "x_train = x_train.reshape(x_train.shape[0], np.prod(x_train.shape[1:]))\n",
        "x_test = x_test / 255\n",
        "x_test = x_test.reshape(x_test.shape[0], np.prod(x_test.shape[1:]))\n",
        "\n",
        "print('Changed Shape (Flattened shape and normalize data as between 0 to 1)')\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JmPo6VPMx10O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class AutoEncoderTester:\n",
        "    def __init__(self, model):\n",
        "        \n",
        "        self.model = model\n",
        "       \n",
        "    def train(self, x_train, y_train, x_test, y_test, epochs=50, batch_size=256, \n",
        "              verbose=1):\n",
        "        self.model.autoencoder.fit(x_train, y_train, \n",
        "                                   validation_data=(x_test, y_test), \n",
        "                                   epochs=epochs, batch_size=batch_size, \n",
        "                                   shuffle=True, \n",
        "                                   verbose=verbose)\n",
        "        \n",
        "    def test(self, x_test):\n",
        "        encoded_imgs = self.model.encoder.predict(x_test)\n",
        "        decoded_imgs = self.model.decoder.predict(encoded_imgs)\n",
        "        \n",
        "        n = 10  # 몇 개의 숫자를 나타낼 것인지\n",
        "        plt.figure(figsize=(20, 4))\n",
        "        for i in range(n):\n",
        "            # 원본 데이터\n",
        "            ax = plt.subplot(2, n, i + 1)\n",
        "            plt.imshow(x_test[i].reshape(28, 28))\n",
        "            plt.gray()\n",
        "            ax.get_xaxis().set_visible(False)\n",
        "            ax.get_yaxis().set_visible(False)\n",
        "\n",
        "            # 재구성된 데이터\n",
        "            ax = plt.subplot(2, n, i + 1 + n)\n",
        "            plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "            plt.gray()\n",
        "            ax.get_xaxis().set_visible(False)\n",
        "            ax.get_yaxis().set_visible(False)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cW4qtlZd27Xg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BaseModel:\n",
        "    def __init__(self):\n",
        "        encode_dim = 32\n",
        "\n",
        "        inputs = Input(shape=(784, ))\n",
        "        encoded = Dense(encode_dim, activation='relu')(inputs)\n",
        "        decoded = Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "        autoencoder = Model(inputs=inputs, outputs=decoded)\n",
        "\n",
        "\n",
        "        encoder = Model(inputs=inputs, outputs=encoded)\n",
        "\n",
        "        encoded_inputs = Input(shape=(encode_dim, ))\n",
        "        decoder_layer = autoencoder.layers[-1]\n",
        "        decoder = Model(inputs=encoded_inputs, outputs=decoder_layer(encoded_inputs))\n",
        "\n",
        "\n",
        "        autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.autoencoder = autoencoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "js7vzr8tyFWG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2573
        },
        "outputId": "2b211240-a518-4fa1-bd6c-b20ec2c4d0f9"
      },
      "cell_type": "code",
      "source": [
        "base_model = AutoEncoderTester(model=BaseModel())\n",
        "base_model.train(x_train=x_train, y_train=x_train, x_test=x_test, y_test=x_test,\n",
        "                epochs=200, batch_size=1024, verbose=1)\n",
        "base_model.test(x_test=x_test)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1144 - val_loss: 0.1126\n",
            "Epoch 133/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1142 - val_loss: 0.1124\n",
            "Epoch 134/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1139 - val_loss: 0.1121\n",
            "Epoch 135/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1137 - val_loss: 0.1119\n",
            "Epoch 136/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1134 - val_loss: 0.1116\n",
            "Epoch 137/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.1132 - val_loss: 0.1114\n",
            "Epoch 138/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.1130 - val_loss: 0.1112\n",
            "Epoch 139/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1127 - val_loss: 0.1109\n",
            "Epoch 140/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1125 - val_loss: 0.1107\n",
            "Epoch 141/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1123 - val_loss: 0.1105\n",
            "Epoch 142/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1121 - val_loss: 0.1103\n",
            "Epoch 143/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1119 - val_loss: 0.1101\n",
            "Epoch 144/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1117 - val_loss: 0.1099\n",
            "Epoch 145/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1115 - val_loss: 0.1097\n",
            "Epoch 146/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1113 - val_loss: 0.1095\n",
            "Epoch 147/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1111 - val_loss: 0.1093\n",
            "Epoch 148/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1109 - val_loss: 0.1091\n",
            "Epoch 149/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1107 - val_loss: 0.1089\n",
            "Epoch 150/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1105 - val_loss: 0.1087\n",
            "Epoch 151/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1103 - val_loss: 0.1085\n",
            "Epoch 152/200\n",
            "60000/60000 [==============================] - 4s 64us/step - loss: 0.1101 - val_loss: 0.1083\n",
            "Epoch 153/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1099 - val_loss: 0.1082\n",
            "Epoch 154/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1098 - val_loss: 0.1080\n",
            "Epoch 155/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1096 - val_loss: 0.1078\n",
            "Epoch 156/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1094 - val_loss: 0.1076\n",
            "Epoch 157/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1093 - val_loss: 0.1075\n",
            "Epoch 158/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1091 - val_loss: 0.1073\n",
            "Epoch 159/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1089 - val_loss: 0.1071\n",
            "Epoch 160/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1088 - val_loss: 0.1070\n",
            "Epoch 161/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1086 - val_loss: 0.1068\n",
            "Epoch 162/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1085 - val_loss: 0.1067\n",
            "Epoch 163/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1083 - val_loss: 0.1065\n",
            "Epoch 164/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1082 - val_loss: 0.1064\n",
            "Epoch 165/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1080 - val_loss: 0.1063\n",
            "Epoch 166/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1079 - val_loss: 0.1061\n",
            "Epoch 167/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1077 - val_loss: 0.1060\n",
            "Epoch 168/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1076 - val_loss: 0.1058\n",
            "Epoch 169/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1075 - val_loss: 0.1057\n",
            "Epoch 170/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1073 - val_loss: 0.1056\n",
            "Epoch 171/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1072 - val_loss: 0.1054\n",
            "Epoch 172/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1071 - val_loss: 0.1053\n",
            "Epoch 173/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1069 - val_loss: 0.1052\n",
            "Epoch 174/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1068 - val_loss: 0.1051\n",
            "Epoch 175/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1067 - val_loss: 0.1049\n",
            "Epoch 176/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1066 - val_loss: 0.1048\n",
            "Epoch 177/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1065 - val_loss: 0.1047\n",
            "Epoch 178/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1063 - val_loss: 0.1046\n",
            "Epoch 179/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1062 - val_loss: 0.1045\n",
            "Epoch 180/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1061 - val_loss: 0.1044\n",
            "Epoch 181/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1060 - val_loss: 0.1043\n",
            "Epoch 182/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1059 - val_loss: 0.1042\n",
            "Epoch 183/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1058 - val_loss: 0.1040\n",
            "Epoch 184/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1057 - val_loss: 0.1039\n",
            "Epoch 185/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1056 - val_loss: 0.1038\n",
            "Epoch 186/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1055 - val_loss: 0.1037\n",
            "Epoch 187/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1054 - val_loss: 0.1036\n",
            "Epoch 188/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1053 - val_loss: 0.1035\n",
            "Epoch 189/200\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.1052 - val_loss: 0.1035\n",
            "Epoch 190/200\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.1051 - val_loss: 0.1034\n",
            "Epoch 191/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.1050 - val_loss: 0.1033\n",
            "Epoch 192/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1049 - val_loss: 0.1032\n",
            "Epoch 193/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1048 - val_loss: 0.1031\n",
            "Epoch 194/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1047 - val_loss: 0.1030\n",
            "Epoch 195/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1047 - val_loss: 0.1029\n",
            "Epoch 196/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1046 - val_loss: 0.1028\n",
            "Epoch 197/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1045 - val_loss: 0.1028\n",
            "Epoch 198/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1044 - val_loss: 0.1027\n",
            "Epoch 199/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1043 - val_loss: 0.1026\n",
            "Epoch 200/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1042 - val_loss: 0.1025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABGoAAADjCAYAAADdR/IFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "-8NGZ-tT0N1n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RegularizedModel:\n",
        "    def __init__(self):\n",
        "        encoding_dim = 32\n",
        "\n",
        "        inputs = Input(shape=(784,))\n",
        "        # L1 activity regularizer를 Dense layer에 추가 \n",
        "        encoded = Dense(encoding_dim, activation='relu',\n",
        "                        activity_regularizer=regularizers.l1(10e-5))(inputs)\n",
        "        decoded = Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "        autoencoder = Model(inputs, decoded)\n",
        "        \n",
        "        encoder = Model(inputs=inputs, outputs=encoded)\n",
        "\n",
        "        encoded_inputs = Input(shape=(encode_dim, ))\n",
        "        decoder_layer = autoencoder.layers[-1]\n",
        "        decoder = Model(inputs=encoded_inputs, outputs=decoder_layer(encoded_inputs))\n",
        "\n",
        "\n",
        "        autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.autoencoder = autoencoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YAhRNWF01RbS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7061
        },
        "outputId": "e85eca18-9cb2-452b-8b4f-3fa97e5b24af"
      },
      "cell_type": "code",
      "source": [
        "regularized_model = AutoEncoderTester(RegularizedModel())\n",
        "regularized_model.train(x_train=x_train, y_train=x_train, x_test=x_test, y_test=x_test,\n",
        "                epochs=200, batch_size=1024, verbose=1)\n",
        "regularized_model.test(x_test=x_test)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.7095 - val_loss: 0.6826\n",
            "Epoch 2/200\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.6765 - val_loss: 0.6704\n",
            "Epoch 3/200\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.6648 - val_loss: 0.6591\n",
            "Epoch 4/200\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.6538 - val_loss: 0.6483\n",
            "Epoch 5/200\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.6431 - val_loss: 0.6378\n",
            "Epoch 6/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.6328 - val_loss: 0.6277\n",
            "Epoch 7/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.6229 - val_loss: 0.6180\n",
            "Epoch 8/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.6134 - val_loss: 0.6087\n",
            "Epoch 9/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.6042 - val_loss: 0.5996\n",
            "Epoch 10/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.5953 - val_loss: 0.5909\n",
            "Epoch 11/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.5868 - val_loss: 0.5826\n",
            "Epoch 12/200\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.5785 - val_loss: 0.5745\n",
            "Epoch 13/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.5706 - val_loss: 0.5667\n",
            "Epoch 14/200\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.5629 - val_loss: 0.5592\n",
            "Epoch 15/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.5556 - val_loss: 0.5519\n",
            "Epoch 16/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.5484 - val_loss: 0.5449\n",
            "Epoch 17/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.5415 - val_loss: 0.5382\n",
            "Epoch 18/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.5349 - val_loss: 0.5317\n",
            "Epoch 19/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.5285 - val_loss: 0.5254\n",
            "Epoch 20/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.5223 - val_loss: 0.5193\n",
            "Epoch 21/200\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.5163 - val_loss: 0.5134\n",
            "Epoch 22/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.5105 - val_loss: 0.5077\n",
            "Epoch 23/200\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.5050 - val_loss: 0.5023\n",
            "Epoch 24/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.4996 - val_loss: 0.4970\n",
            "Epoch 25/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.4944 - val_loss: 0.4918\n",
            "Epoch 26/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.4893 - val_loss: 0.4869\n",
            "Epoch 27/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.4844 - val_loss: 0.4821\n",
            "Epoch 28/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.4797 - val_loss: 0.4774\n",
            "Epoch 29/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.4751 - val_loss: 0.4729\n",
            "Epoch 30/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.4707 - val_loss: 0.4686\n",
            "Epoch 31/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.4664 - val_loss: 0.4644\n",
            "Epoch 32/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.4623 - val_loss: 0.4603\n",
            "Epoch 33/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.4583 - val_loss: 0.4563\n",
            "Epoch 34/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.4544 - val_loss: 0.4525\n",
            "Epoch 35/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.4506 - val_loss: 0.4488\n",
            "Epoch 36/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.4469 - val_loss: 0.4452\n",
            "Epoch 37/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.4434 - val_loss: 0.4417\n",
            "Epoch 38/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.4399 - val_loss: 0.4383\n",
            "Epoch 39/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.4366 - val_loss: 0.4350\n",
            "Epoch 40/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.4333 - val_loss: 0.4318\n",
            "Epoch 41/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.4302 - val_loss: 0.4287\n",
            "Epoch 42/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.4271 - val_loss: 0.4257\n",
            "Epoch 43/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.4241 - val_loss: 0.4228\n",
            "Epoch 44/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.4213 - val_loss: 0.4199\n",
            "Epoch 45/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.4184 - val_loss: 0.4172\n",
            "Epoch 46/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.4157 - val_loss: 0.4145\n",
            "Epoch 47/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.4131 - val_loss: 0.4119\n",
            "Epoch 48/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.4105 - val_loss: 0.4093\n",
            "Epoch 49/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.4080 - val_loss: 0.4068\n",
            "Epoch 50/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.4055 - val_loss: 0.4044\n",
            "Epoch 51/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.4032 - val_loss: 0.4021\n",
            "Epoch 52/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.4008 - val_loss: 0.3998\n",
            "Epoch 53/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3986 - val_loss: 0.3976\n",
            "Epoch 54/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3964 - val_loss: 0.3954\n",
            "Epoch 55/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3943 - val_loss: 0.3933\n",
            "Epoch 56/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3922 - val_loss: 0.3913\n",
            "Epoch 57/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3902 - val_loss: 0.3893\n",
            "Epoch 58/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3882 - val_loss: 0.3873\n",
            "Epoch 59/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3862 - val_loss: 0.3854\n",
            "Epoch 60/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3844 - val_loss: 0.3836\n",
            "Epoch 61/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3825 - val_loss: 0.3818\n",
            "Epoch 62/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3807 - val_loss: 0.3800\n",
            "Epoch 63/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3790 - val_loss: 0.3783\n",
            "Epoch 64/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.3773 - val_loss: 0.3766\n",
            "Epoch 65/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.3756 - val_loss: 0.3750\n",
            "Epoch 66/200\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.3740 - val_loss: 0.3734\n",
            "Epoch 67/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3724 - val_loss: 0.3718\n",
            "Epoch 68/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3709 - val_loss: 0.3703\n",
            "Epoch 69/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3694 - val_loss: 0.3688\n",
            "Epoch 70/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3679 - val_loss: 0.3673\n",
            "Epoch 71/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3665 - val_loss: 0.3659\n",
            "Epoch 72/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3650 - val_loss: 0.3645\n",
            "Epoch 73/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3637 - val_loss: 0.3631\n",
            "Epoch 74/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3623 - val_loss: 0.3618\n",
            "Epoch 75/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3610 - val_loss: 0.3605\n",
            "Epoch 76/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3597 - val_loss: 0.3592\n",
            "Epoch 77/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3585 - val_loss: 0.3580\n",
            "Epoch 78/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3572 - val_loss: 0.3568\n",
            "Epoch 79/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3560 - val_loss: 0.3556\n",
            "Epoch 80/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3548 - val_loss: 0.3544\n",
            "Epoch 81/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3537 - val_loss: 0.3533\n",
            "Epoch 82/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3525 - val_loss: 0.3521\n",
            "Epoch 83/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3514 - val_loss: 0.3510\n",
            "Epoch 84/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3504 - val_loss: 0.3500\n",
            "Epoch 85/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3493 - val_loss: 0.3489\n",
            "Epoch 86/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3482 - val_loss: 0.3479\n",
            "Epoch 87/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3472 - val_loss: 0.3469\n",
            "Epoch 88/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3462 - val_loss: 0.3459\n",
            "Epoch 89/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3452 - val_loss: 0.3449\n",
            "Epoch 90/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3443 - val_loss: 0.3440\n",
            "Epoch 91/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3433 - val_loss: 0.3430\n",
            "Epoch 92/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3424 - val_loss: 0.3421\n",
            "Epoch 93/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3415 - val_loss: 0.3412\n",
            "Epoch 94/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3406 - val_loss: 0.3403\n",
            "Epoch 95/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3398 - val_loss: 0.3395\n",
            "Epoch 96/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3389 - val_loss: 0.3386\n",
            "Epoch 97/200\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.3381 - val_loss: 0.3378\n",
            "Epoch 98/200\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.3372 - val_loss: 0.3370\n",
            "Epoch 99/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3364 - val_loss: 0.3362\n",
            "Epoch 100/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3356 - val_loss: 0.3354\n",
            "Epoch 101/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3349 - val_loss: 0.3346\n",
            "Epoch 102/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3341 - val_loss: 0.3339\n",
            "Epoch 103/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3334 - val_loss: 0.3331\n",
            "Epoch 104/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3326 - val_loss: 0.3324\n",
            "Epoch 105/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3319 - val_loss: 0.3317\n",
            "Epoch 106/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3312 - val_loss: 0.3310\n",
            "Epoch 107/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3305 - val_loss: 0.3303\n",
            "Epoch 108/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3298 - val_loss: 0.3296\n",
            "Epoch 109/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3291 - val_loss: 0.3289\n",
            "Epoch 110/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3285 - val_loss: 0.3283\n",
            "Epoch 111/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3278 - val_loss: 0.3276\n",
            "Epoch 112/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3272 - val_loss: 0.3270\n",
            "Epoch 113/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3265 - val_loss: 0.3264\n",
            "Epoch 114/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3259 - val_loss: 0.3258\n",
            "Epoch 115/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3253 - val_loss: 0.3252\n",
            "Epoch 116/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3247 - val_loss: 0.3246\n",
            "Epoch 117/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3241 - val_loss: 0.3240\n",
            "Epoch 118/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3236 - val_loss: 0.3234\n",
            "Epoch 119/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3230 - val_loss: 0.3228\n",
            "Epoch 120/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3224 - val_loss: 0.3223\n",
            "Epoch 121/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3219 - val_loss: 0.3217\n",
            "Epoch 122/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3213 - val_loss: 0.3212\n",
            "Epoch 123/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3208 - val_loss: 0.3207\n",
            "Epoch 124/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3203 - val_loss: 0.3202\n",
            "Epoch 125/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3198 - val_loss: 0.3196\n",
            "Epoch 126/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3192 - val_loss: 0.3191\n",
            "Epoch 127/200\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.3187 - val_loss: 0.3186\n",
            "Epoch 128/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3183 - val_loss: 0.3181\n",
            "Epoch 129/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3178 - val_loss: 0.3177\n",
            "Epoch 130/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3173 - val_loss: 0.3172\n",
            "Epoch 131/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3168 - val_loss: 0.3167\n",
            "Epoch 132/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3164 - val_loss: 0.3163\n",
            "Epoch 133/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3159 - val_loss: 0.3158\n",
            "Epoch 134/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3154 - val_loss: 0.3154\n",
            "Epoch 135/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3150 - val_loss: 0.3149\n",
            "Epoch 136/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3146 - val_loss: 0.3145\n",
            "Epoch 137/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3141 - val_loss: 0.3141\n",
            "Epoch 138/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3137 - val_loss: 0.3136\n",
            "Epoch 139/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3133 - val_loss: 0.3132\n",
            "Epoch 140/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3129 - val_loss: 0.3128\n",
            "Epoch 141/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.3125 - val_loss: 0.3124\n",
            "Epoch 142/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.3121 - val_loss: 0.3120\n",
            "Epoch 143/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3117 - val_loss: 0.3116\n",
            "Epoch 144/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3113 - val_loss: 0.3112\n",
            "Epoch 145/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3109 - val_loss: 0.3108\n",
            "Epoch 146/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3105 - val_loss: 0.3105\n",
            "Epoch 147/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.3102 - val_loss: 0.3101\n",
            "Epoch 148/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3098 - val_loss: 0.3097\n",
            "Epoch 149/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3094 - val_loss: 0.3094\n",
            "Epoch 150/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3091 - val_loss: 0.3090\n",
            "Epoch 151/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3087 - val_loss: 0.3087\n",
            "Epoch 152/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3084 - val_loss: 0.3083\n",
            "Epoch 153/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3080 - val_loss: 0.3080\n",
            "Epoch 154/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3077 - val_loss: 0.3076\n",
            "Epoch 155/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3073 - val_loss: 0.3073\n",
            "Epoch 156/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3070 - val_loss: 0.3070\n",
            "Epoch 157/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3067 - val_loss: 0.3067\n",
            "Epoch 158/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3064 - val_loss: 0.3063\n",
            "Epoch 159/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3061 - val_loss: 0.3060\n",
            "Epoch 160/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3057 - val_loss: 0.3057\n",
            "Epoch 161/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3054 - val_loss: 0.3054\n",
            "Epoch 162/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3051 - val_loss: 0.3051\n",
            "Epoch 163/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3048 - val_loss: 0.3048\n",
            "Epoch 164/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3045 - val_loss: 0.3045\n",
            "Epoch 165/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3042 - val_loss: 0.3042\n",
            "Epoch 166/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3039 - val_loss: 0.3039\n",
            "Epoch 167/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.3037 - val_loss: 0.3036\n",
            "Epoch 168/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3034 - val_loss: 0.3033\n",
            "Epoch 169/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3031 - val_loss: 0.3031\n",
            "Epoch 170/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3028 - val_loss: 0.3028\n",
            "Epoch 171/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3025 - val_loss: 0.3025\n",
            "Epoch 172/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3023 - val_loss: 0.3022\n",
            "Epoch 173/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3020 - val_loss: 0.3020\n",
            "Epoch 174/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3017 - val_loss: 0.3017\n",
            "Epoch 175/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3015 - val_loss: 0.3015\n",
            "Epoch 176/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.3012 - val_loss: 0.3012\n",
            "Epoch 177/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3010 - val_loss: 0.3009\n",
            "Epoch 178/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3007 - val_loss: 0.3007\n",
            "Epoch 179/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.3005 - val_loss: 0.3004\n",
            "Epoch 180/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3002 - val_loss: 0.3002\n",
            "Epoch 181/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.3000 - val_loss: 0.3000\n",
            "Epoch 182/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.2997 - val_loss: 0.2997\n",
            "Epoch 183/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.2995 - val_loss: 0.2995\n",
            "Epoch 184/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.2993 - val_loss: 0.2993\n",
            "Epoch 185/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.2990 - val_loss: 0.2990\n",
            "Epoch 186/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.2988 - val_loss: 0.2988\n",
            "Epoch 187/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.2986 - val_loss: 0.2986\n",
            "Epoch 188/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.2984 - val_loss: 0.2983\n",
            "Epoch 189/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.2981 - val_loss: 0.2981\n",
            "Epoch 190/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.2979 - val_loss: 0.2979\n",
            "Epoch 191/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.2977 - val_loss: 0.2977\n",
            "Epoch 192/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.2975 - val_loss: 0.2975\n",
            "Epoch 193/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.2973 - val_loss: 0.2973\n",
            "Epoch 194/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.2971 - val_loss: 0.2971\n",
            "Epoch 195/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.2969 - val_loss: 0.2968\n",
            "Epoch 196/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.2967 - val_loss: 0.2966\n",
            "Epoch 197/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.2965 - val_loss: 0.2964\n",
            "Epoch 198/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.2962 - val_loss: 0.2962\n",
            "Epoch 199/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.2961 - val_loss: 0.2960\n",
            "Epoch 200/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.2959 - val_loss: 0.2958\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABGoAAADjCAYAAADdR/IFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Mmas2xDL1Uci",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DeepModel:\n",
        "    def __init__(self):\n",
        "        encoding_dim = 32\n",
        "\n",
        "        inputs = Input(shape=(784,))\n",
        "        encoded = Dense(128, activation='relu')(inputs)\n",
        "        encoded = Dense(64, activation='relu')(encoded)\n",
        "        encoded = Dense(32, activation='relu')(encoded)\n",
        "\n",
        "        decoded = Dense(64, activation='relu')(encoded)\n",
        "        decoded = Dense(128, activation='relu')(decoded)\n",
        "        decoded = Dense(784, activation='sigmoid')(decoded)\n",
        "\n",
        "        autoencoder = Model(inputs, decoded)\n",
        "        \n",
        "        encoder = Model(inputs=inputs, outputs=encoded)\n",
        "\n",
        "        encoded_inputs = Input(shape=(encode_dim, ))\n",
        "        decoder_layer = autoencoder.layers[-1]\n",
        "        decoder = Model(inputs=encoded_inputs, outputs=decoder_layer(encoded_inputs))\n",
        "\n",
        "\n",
        "        autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.autoencoder = autoencoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zG70k1EL7eNV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "501403f2-25a1-44d5-e3db-c2a6bfa9d83e"
      },
      "cell_type": "code",
      "source": [
        "deep_model = AutoEncoderTester(DeepModel())\n",
        "deep_model.train(x_train=x_train, y_train=x_train, x_test=x_test, y_test=x_test,\n",
        "                epochs=200, batch_size=1024, verbose=1)\n",
        "deep_model.test(x_test=x_test)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-aeeecc71a0e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdeep_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoEncoderTester\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDeepModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m deep_model.train(x_train=x_train, y_train=x_train, x_test=x_test, y_test=x_test,\n\u001b[1;32m      3\u001b[0m                 epochs=200, batch_size=1024, verbose=1)\n\u001b[1;32m      4\u001b[0m \u001b[0mdeep_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-9eef4ef939a9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mencoded_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdecoder_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoded_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;31m# with the input_spec set at build time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;31m# Handle mask propagation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    350\u001b[0m                                 \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' of input shape to have '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                                 \u001b[0;34m'value '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                                 ' but got shape ' + str(x_shape))\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;31m# Check shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer dense_23: expected axis -1 of input shape to have value 128 but got shape (None, 32)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "D0roSg9z7ia3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvModel:\n",
        "    def __init__(self):\n",
        "        encoding_dim = 32\n",
        "\n",
        "        inputs = Input(shape=(28, 28, 1))  # 'channels_firtst'이미지 데이터 형식을 사용하는 경우 이를 적용\n",
        "\n",
        "        x = Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
        "        x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "        x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "        x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "        x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "        encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "        # 이 시점에서 표현(representatoin)은 (4,4,8) 즉, 128 차원\n",
        "\n",
        "        x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "        x = UpSampling2D((2, 2))(x)\n",
        "        x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "        x = UpSampling2D((2, 2))(x)\n",
        "        x = Conv2D(16, (3, 3), activation='relu')(x)\n",
        "        x = UpSampling2D((2, 2))(x)\n",
        "        decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "        autoencoder = Model(inputs, decoded)\n",
        "        \n",
        "        encoder = Model(inputs=inputs, outputs=encoded)\n",
        "\n",
        "        encoded_inputs = Input(shape=(encode_dim, ))\n",
        "        decoder_layer = autoencoder.layers[-1]\n",
        "        decoder = Model(inputs=encoded_inputs, outputs=decoder_layer(encoded_inputs))\n",
        "\n",
        "\n",
        "        autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.autoencoder = autoencoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LclqdAUy8WXi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reshape for conv2d\n",
        "x_train_3d = np.reshape(x_train, (len(x_train), 28, 28, 1))  # 'channels_firtst'이미지 데이터 형식을 사용하는 경우 이를 적용\n",
        "x_test_3d = np.reshape(x_test, (len(x_test), 28, 28, 1))  # 'channels_firtst'이미지 데이터 형식을 사용하는 경우 이를 적용\n",
        "\n",
        "conv_model = AutoEncoderTester(ConvModel())\n",
        "conv_model.train(x_train=x_train_3d, y_train=x_train_3d, x_test=x_test_3d, y_test=x_test_3d,\n",
        "                epochs=200, batch_size=1024, verbose=1)\n",
        "conv_model.test(x_test=x_test_3d)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}